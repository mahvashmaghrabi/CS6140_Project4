{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34278935",
   "metadata": {},
   "source": [
    "# Project 4: Introduction to Deep Learning\n",
    "## Name - Mahvash Maghrabi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37e73b6",
   "metadata": {},
   "source": [
    " ## Task 2 - Experiment with Network Variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "ee657fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.transforms import transforms\n",
    "import copy\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "e6fdcd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the MNIST Fashion dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "trainset = torchvision.datasets.FashionMNIST(root='/Users/mahvashmaghrabi/Desktop/CS6140/MNISTFashion', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.FashionMNIST(root='/Users/mahvashmaghrabi/Desktop/CS6140/MNISTFashion', train=False, download=True, transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "252a125e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the network\n",
    "# NetEstimator class extends Net class which is a neural network model\n",
    "class NetEstimator(Net, BaseEstimator):\n",
    "    def __init__(self, n_conv_layers, conv_filter_size, n_conv_filters, dense_nodes, dropout_rate, pool_filter_size, activation_func, batch_size=32, epochs=10, learning_rate=0.001):\n",
    "        super(NetEstimator, self).__init__(n_conv_layers, conv_filter_size, n_conv_filters, dense_nodes, dropout_rate, pool_filter_size, activation_func)\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "# This function trains the neural network using Adam optimizer and cross-entropy loss function    \n",
    "    def fit(self, X, y=None):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        lossfunction = nn.CrossEntropyLoss()\n",
    "        train_loader = DataLoader(X, batch_size=self.batch_size, shuffle=True)\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            for i, data in enumerate(train_loader, 0):\n",
    "                inputs, labels = data\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(inputs)\n",
    "                loss = lossfunction(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "# This function makes predictions on the input data using the trained model                                \n",
    "    def predict(self, X):\n",
    "        test_loader = DataLoader(X, batch_size=self.batch_size, shuffle=False)\n",
    "        with torch.no_grad():\n",
    "            predictions = []\n",
    "            for inputs in test_loader:\n",
    "                outputs = self(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                predictions.append(predicted.numpy())\n",
    "            predictions = np.concatenate(predictions)\n",
    "        return predictions\n",
    "\n",
    "# This function calculates the accuracy of the model predictions\n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        return accuracy_score(y, y_pred)\n",
    "\n",
    "# This function returns a dictionary of the hyperparameters     \n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            'n_conv_layers': self.n_conv_layers,\n",
    "            'conv_filter_size': self.conv_filter_size,\n",
    "            'n_conv_filters': self.n_conv_filters,\n",
    "            'dense_nodes': self.dense_nodes,\n",
    "            'dropout_rate': self.dropout_rate,\n",
    "            'pool_filter_size': self.pool_filter_size,\n",
    "            'activation_func': self.activation_func,\n",
    "            'batch_size': self.batch_size,\n",
    "            'epochs': self.epochs,\n",
    "            'learning_rate': self.learning_rate\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "86df354d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the parameter grid for hyperparameter tuning of GridSearch CV\n",
    "param_grid = {\n",
    "    'n_conv_layers': [2, 3],\n",
    "    'conv_filter_size': [3, 5],\n",
    "    'n_conv_filters': [16, 32],\n",
    "    'dense_nodes': [128, 256],\n",
    "    'dropout_rate': [0.1, 0.3],\n",
    "    'pool_filter_size': [2],\n",
    "    'activation_func': ['relu', 'sigmoid', 'tanh']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "c4d2bae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the cross-validation value\n",
    "cv = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "1c5bcb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the data loaders for the original dataset\n",
    "train_loader = DataLoader(trainset, batch_size=100, shuffle=True)\n",
    "test_loader = DataLoader(testset, batch_size=100, shuffle=True)\n",
    "\n",
    "# Splitting the training set into training sets and validation sets\n",
    "train_size = int(0.8 * len(trainset))\n",
    "val_size = len(trainset) - train_size\n",
    "trainset, val_dataset = random_split(trainset, [train_size, val_size])\n",
    "\n",
    "# Defining the data loaders for both the training and validation sets\n",
    "train_loader = DataLoader(trainset, batch_size=100, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "c8bde3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the training function\n",
    "def train(model, train_loader, val_loader, criterion, optimizer, epochs):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "        model.train()\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * data.size(0)\n",
    "        \n",
    "        model.eval()\n",
    "        for data, target in val_loader:\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            val_loss += loss.item() * data.size(0)\n",
    "        \n",
    "        train_loss = train_loss / len(train_loader.dataset)\n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch+1, train_loss, val_loss))\n",
    "    \n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "72391288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the test function\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    test_acc = 0.0\n",
    "    for data, target in test_loader:\n",
    "        output = model(data)\n",
    "        _, pred = torch.max(output, 1)\n",
    "        test_acc += accuracy_score(target.numpy(), pred.numpy(), normalize=False)\n",
    "    \n",
    "    test_acc = test_acc / len(test_loader.dataset)\n",
    "    print('Test Accuracy: {:.6f}%'.format(100*test_acc))\n",
    "    \n",
    "    return test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "ce3e4f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the neural network model\n",
    "model = Net(n_conv_layers=2, conv_filter_size=3, n_conv_filters=16, dense_nodes=128, dropout_rate=0.1, pool_filter_size=2, activation_func='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "4bc7f6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the loss function and the optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "42bf68f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.637946 \tValidation Loss: 0.445476\n",
      "Epoch: 2 \tTraining Loss: 0.411614 \tValidation Loss: 0.380074\n",
      "Epoch: 3 \tTraining Loss: 0.358836 \tValidation Loss: 0.346245\n",
      "Epoch: 4 \tTraining Loss: 0.329728 \tValidation Loss: 0.348336\n",
      "Epoch: 5 \tTraining Loss: 0.305989 \tValidation Loss: 0.319920\n",
      "Epoch: 6 \tTraining Loss: 0.286590 \tValidation Loss: 0.313371\n",
      "Epoch: 7 \tTraining Loss: 0.275302 \tValidation Loss: 0.296243\n",
      "Epoch: 8 \tTraining Loss: 0.261209 \tValidation Loss: 0.296186\n",
      "Epoch: 9 \tTraining Loss: 0.251139 \tValidation Loss: 0.290870\n",
      "Epoch: 10 \tTraining Loss: 0.238688 \tValidation Loss: 0.288106\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "train_losses, val_losses = train(model, train_loader, val_loader, criterion, optimizer, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "877dbbd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 89.810000%\n"
     ]
    }
   ],
   "source": [
    "# Testing the model accuracy\n",
    "test_acc = test(model, test_loader)\n",
    "#print('Test Accuracy: {:.6f}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "cad2f90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=NetEstimator(n_conv_layers, conv_filter_size, n_conv_filters, dense_nodes, dropout_rate, pool_filter_size, activation_func),\n",
    "                           param_grid=param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=cv,\n",
    "                           n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "cf5482b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "288 fits failed out of a total of 288.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "72 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\", line 678, in _fit_and_score\n",
      "    estimator.fit(X_train, **fit_params)\n",
      "  File \"/var/folders/qf/z5th9xgd4vsfl5hf0qbyntdr0000gn/T/ipykernel_4425/1980059986.py\", line 17, in fit\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/var/folders/qf/z5th9xgd4vsfl5hf0qbyntdr0000gn/T/ipykernel_4425/429576448.py\", line 36, in forward\n",
      "RuntimeError: shape '[-1, 400]' is invalid for input of size 8192\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "144 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\", line 678, in _fit_and_score\n",
      "    estimator.fit(X_train, **fit_params)\n",
      "  File \"/var/folders/qf/z5th9xgd4vsfl5hf0qbyntdr0000gn/T/ipykernel_4425/1980059986.py\", line 17, in fit\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/var/folders/qf/z5th9xgd4vsfl5hf0qbyntdr0000gn/T/ipykernel_4425/429576448.py\", line 33, in forward\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/nn/modules/conv.py\", line 463, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/nn/modules/conv.py\", line 460, in _conv_forward\n",
      "    self.padding, self.dilation, self.groups)\n",
      "RuntimeError: Calculated padded input size per channel: (2 x 2). Kernel size: (3 x 3). Kernel size can't be greater than actual input size\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "72 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\", line 678, in _fit_and_score\n",
      "    estimator.fit(X_train, **fit_params)\n",
      "  File \"/var/folders/qf/z5th9xgd4vsfl5hf0qbyntdr0000gn/T/ipykernel_4425/1980059986.py\", line 17, in fit\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/var/folders/qf/z5th9xgd4vsfl5hf0qbyntdr0000gn/T/ipykernel_4425/429576448.py\", line 36, in forward\n",
      "RuntimeError: shape '[-1, 800]' is invalid for input of size 8192\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan]\n",
      "  category=UserWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=NetEstimator(activation_func='relu', conv_filter_size=3, dense_nodes=64, dropout_rate=0.1, n_conv_filters=64, n_conv_layers=3, pool_filter_size=3),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'activation_func': ['relu', 'sigmoid', 'tanh'],\n",
       "                         'conv_filter_size': [3, 5], 'dense_nodes': [128, 256],\n",
       "                         'dropout_rate': [0.1, 0.3], 'n_conv_filters': [16, 32],\n",
       "                         'n_conv_layers': [2, 3], 'pool_filter_size': [2]},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the Grid Search CV\n",
    "grid_search.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "8f50b505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:  {'activation_func': 'relu', 'conv_filter_size': 3, 'dense_nodes': 128, 'dropout_rate': 0.1, 'n_conv_filters': 16, 'n_conv_layers': 2, 'pool_filter_size': 2}\n"
     ]
    }
   ],
   "source": [
    "# Printing the best hyperparameters\n",
    "print(\"Best hyperparameters: \", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "f8bda196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] Train Loss: 0.6557 Train Acc: 76.00% Val Loss: 0.4652 Val Acc: 82.99%\n",
      "Epoch [2/20] Train Loss: 0.4204 Train Acc: 84.59% Val Loss: 0.3890 Val Acc: 85.70%\n",
      "Epoch [3/20] Train Loss: 0.3634 Train Acc: 86.75% Val Loss: 0.3506 Val Acc: 86.86%\n",
      "Epoch [4/20] Train Loss: 0.3283 Train Acc: 87.96% Val Loss: 0.3295 Val Acc: 87.58%\n",
      "Epoch [5/20] Train Loss: 0.3035 Train Acc: 88.90% Val Loss: 0.3403 Val Acc: 87.07%\n",
      "Epoch [6/20] Train Loss: 0.2864 Train Acc: 89.54% Val Loss: 0.3173 Val Acc: 88.08%\n",
      "Epoch [7/20] Train Loss: 0.2697 Train Acc: 90.00% Val Loss: 0.3100 Val Acc: 88.56%\n",
      "Epoch [8/20] Train Loss: 0.2617 Train Acc: 90.42% Val Loss: 0.3000 Val Acc: 88.78%\n",
      "Epoch [9/20] Train Loss: 0.2487 Train Acc: 90.75% Val Loss: 0.2884 Val Acc: 89.28%\n",
      "Epoch [10/20] Train Loss: 0.2358 Train Acc: 91.32% Val Loss: 0.2885 Val Acc: 89.60%\n",
      "Epoch [11/20] Train Loss: 0.2279 Train Acc: 91.62% Val Loss: 0.2894 Val Acc: 89.33%\n",
      "Epoch [12/20] Train Loss: 0.2192 Train Acc: 91.82% Val Loss: 0.2977 Val Acc: 89.28%\n",
      "Epoch [13/20] Train Loss: 0.2111 Train Acc: 92.26% Val Loss: 0.2869 Val Acc: 89.54%\n",
      "Epoch [14/20] Train Loss: 0.2064 Train Acc: 92.17% Val Loss: 0.2807 Val Acc: 89.89%\n",
      "Epoch [15/20] Train Loss: 0.1983 Train Acc: 92.55% Val Loss: 0.2918 Val Acc: 89.78%\n",
      "Epoch [16/20] Train Loss: 0.1900 Train Acc: 92.79% Val Loss: 0.3026 Val Acc: 89.82%\n",
      "Epoch [17/20] Train Loss: 0.1847 Train Acc: 93.14% Val Loss: 0.3048 Val Acc: 89.57%\n",
      "Epoch [18/20] Train Loss: 0.1814 Train Acc: 93.10% Val Loss: 0.2931 Val Acc: 89.49%\n",
      "Epoch [19/20] Train Loss: 0.1751 Train Acc: 93.55% Val Loss: 0.2838 Val Acc: 90.19%\n",
      "Epoch [20/20] Train Loss: 0.1689 Train Acc: 93.64% Val Loss: 0.3091 Val Acc: 89.72%\n"
     ]
    }
   ],
   "source": [
    "# Training the model using the best hyperparameter values from Grid Search CV\n",
    "# Defining the neural network\n",
    "model = Net(n_conv_layers=2, conv_filter_size=3, n_conv_filters=16,\n",
    "            pool_filter_size=2, dense_nodes=128, dropout_rate=0.1,\n",
    "            activation_func='relu')\n",
    "\n",
    "# Defining the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training the network with number of epochs = 20\n",
    "num_epochs = 20\n",
    "best_val_acc = 0.0\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_acc = 100 * correct / total\n",
    "\n",
    "# Evaluating the performance\n",
    "    val_loss, val_acc = test(model, val_loader)\n",
    "\n",
    "\n",
    "# Saving the best model based on accuracy\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_model = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {train_loss:.4f} \"\n",
    "          f\"Train Acc: {train_acc:.2f}% Val Loss: {val_loss:.4f} Val Acc: {val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "c68ccb83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2838 Test Accuracy: 90.19%\n"
     ]
    }
   ],
   "source": [
    "#  Printing the Test Accuracy\n",
    "model.load_state_dict(best_model)\n",
    "test_loss, test_acc = test(model, val_loader)\n",
    "print(f\"Test Loss: {test_loss:.4f} Test Accuracy: {test_acc:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
